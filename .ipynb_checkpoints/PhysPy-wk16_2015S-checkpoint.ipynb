{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick PCA review: The difference between \n",
    "\n",
    "### pca.fit() \n",
    "### pca.transform()\n",
    "### pca.fit_transform()\n",
    "\n",
    "## 2. Suppoort Vector Machine (SVM)\n",
    "\n",
    "## 3. SVM Applied to Handwritten Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC: Support Vector Classification\n",
    "## In sklearn.svm.SVC() the default kernel is RBF\n",
    "## For kernel parameters, see\n",
    "## http://scikit-learn.org/stable/modules/svm.html#svm-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'> float64 (1797, 64)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'digits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fca69f95aefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'digits' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "''' Initial Imports'''\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# ***use seaborn plotting style defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "#********************* KEY IMPORT OF THIS LECTURE********************************\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "# loading handwritten digits\n",
    "dig_data = load_digits()\n",
    "X = dig_data.data\n",
    "# y: the values of the digits, or \"ground truth\"\n",
    "y = dig_data.target\n",
    "\n",
    "print(type(X), X.dtype, X.shape)\n",
    "print(type(digits), digits.dtype, digits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Simple linear example.  Based on code by \n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "with significant modification.\n",
    "\n",
    "This cell is heavily annotated with comments to facilitate your learning\n",
    "-- please study carefully...and ask me questions if anything is unclear!\n",
    "\n",
    "\n",
    "[Below I provide two examples of more sophisticaed python coding -- \n",
    "it's not required, but if you are interested:\n",
    "\n",
    "1. numpy.c_ and numpy.r_\n",
    "http://docs.scipy.org/doc/numpy-1.8.1/reference/generated/numpy.c_.html\n",
    "http://stackoverflow.com/questions/18601001/numpy-r-is-not-a-function-what-is-it\n",
    "\n",
    "In particular:\n",
    "\"[np.c_] is in reality not a function, but a class instance of RClass, which has __getindex__ implemented, \n",
    "so that you can use it as r_[1]. The cosmetic difference is that you use square brackets instead of curved ones, \n",
    "so you are not doing a function call, but you are actually indexing the object.\"]\n",
    "\n",
    "2. numpy.mgrid\n",
    "\n",
    "It's a similar idea:\n",
    "\n",
    "http://students.mimuw.edu.pl/~pbechler/numpy_doc/reference/generated/numpy.mgrid.html\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# If you don't feel comfortable using the following\n",
    "# then use np.column_stack()\n",
    "# X = np.c_[(0.1, 0.9),\n",
    "#           (0.9, 0.1),\n",
    "#           #-- separating the two groups.\n",
    "#           (1, 2),\n",
    "#           (2, 3)].T\n",
    "\n",
    "# dataset \n",
    "X = np.column_stack([(0.1, 0.9),\n",
    "          (0.9, 0.1),\n",
    "          #-- separating the two groups.\n",
    "          (1, 2),\n",
    "          (2, 3)]).T\n",
    "\n",
    "# targets (or \"ground truth\")\n",
    "Y = [-1] * 2 + [1] * 2\n",
    "\n",
    "\n",
    "# fit the model; clf: classifier\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# plot the line, the points\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# zorder = 10 (\"high\") makes sure that the points are not covered by what's plotted below.\n",
    "plt.scatter(X[:2, 0], X[:2, 1], marker = '_', c = 'r', s = 100, lw = 2, zorder = 10)\n",
    "plt.scatter(X[2:, 0], X[2:, 1], marker = '+', c = 'r', s = 100, lw = 2, zorder = 10)\n",
    "\n",
    "x_min = -0.1\n",
    "x_max = 3\n",
    "y_min = -0.1\n",
    "y_max = 3.5\n",
    "\n",
    "# A more sophisticated way (see docsting)\n",
    "# XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "x = np.linspace(x_min, x_max, 200)\n",
    "y = np.linspace(y_min, y_max, 200)\n",
    "XX, YY = np.meshgrid(x, y)\n",
    "\n",
    "# The .flatten() method flattens a 2D array into a 1D array.\n",
    "# To restore the original 2D shape, one can use the .reshape() method.\n",
    "Z = clf.decision_function(np.column_stack([XX.flatten(), YY.flatten()]))\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "# classification map -- a binary array:\n",
    "# class_map = 0 for the -ve region, and\n",
    "# class_map = 1 for the +ve region.\n",
    "class_map = Z > 0\n",
    "\n",
    "# because the grid of XX and YY are not integers, \n",
    "# one has to explicitly specify the size of the image.\n",
    "plt.imshow(class_map, cmap= plt.cm.Paired, \n",
    "           extent=[x_min, x_max, y_min, y_max], origin='lower')\n",
    "\n",
    "plt.grid('off')\n",
    "\n",
    "# Instead of imshow(), you can also do the following:\n",
    "# plt.pcolormesh(XX, YY, classmap, cmap= plt.cm.Paired)\n",
    "\n",
    "# color levels: \n",
    "# 0: meridian; -1: lower boundary; +1: upper boundary.\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "            levels=[-1, 0, 1])\n",
    "\n",
    "# Setting limits for x and y axes.\n",
    "plt.axis((x_min, x_max, y_min, y_max))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Linear kernel NOT sufficiet...\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "# New dataset and targets\n",
    "X = np.column_stack([(0.1, 0.9),\n",
    "          (2.5, 1),\n",
    "          #-- separating the two groups.\n",
    "          (2, 3),\n",
    "          (0.9, 0.1)]).T\n",
    "Y = [-1] * 2 + [1] * 2\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "plt.scatter(X[:2, 0], X[:2, 1], marker = '_', c = 'k', s = 100, lw = 2, zorder = 10)\n",
    "plt.scatter(X[2:, 0], X[2:, 1], marker = '+', c = 'k', s = 100, lw = 2, zorder = 10)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:2, 0], X[:2, 1], marker = '_', c = 'r', s = 100, lw = 2, zorder = 10)\n",
    "plt.scatter(X[2:, 0], X[2:, 1], marker = '+', c = 'r', s = 100, lw = 2, zorder = 10)\n",
    "\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "            levels=[-1, 0, 1])\n",
    "\n",
    "plt.axis((x_min, x_max, y_min, y_max))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Linear kernel NOT sufficiet...use polynomial kernel.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Same data and targets as above cell.\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "plt.scatter(X[:2, 0], X[:2, 1], marker = '_', c = 'k', s = 100, lw = 2, zorder = 10)\n",
    "plt.scatter(X[2:, 0], X[2:, 1], marker = '+', c = 'k', s = 100, lw = 2, zorder = 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fit the model\n",
    "# degree = 1 is still linear\n",
    "clf = svm.SVC(kernel='poly', degree = 1, gamma = 2)  # nonlinear, still not nonlinear enough\n",
    "\n",
    "# default: degree = 3, gamma = 1\n",
    "#clf = svm.SVC(kernel='poly', gamma = 2)  # nonlinear, still not nonlinear enough\n",
    "\n",
    "# Yay!!\n",
    "#clf = svm.SVC(kernel='poly', gamma = 3)  \n",
    "\n",
    "clf.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:2, 0], X[:2, 1], marker = '_', c = 'r', s = 100, lw = 2, zorder = 10)\n",
    "plt.scatter(X[2:, 0], X[2:, 1], marker = '+', c = 'r', s = 100, lw = 2, zorder = 10)\n",
    "\n",
    "\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "            levels=[-1, 0, 1])\n",
    "\n",
    "plt.axis((x_min, x_max, y_min, y_max))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Linear kernel NOT sufficiet...use polynomial kernel; still not good enough...try rbf\n",
    "\n",
    "You don't see the \"gutter\" because you need more than four points.  Feel free to come up with your own examples.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(X[:2, 0], X[:2, 1], marker = '_', c = 'k', s = 100, lw = 2, zorder = 10)\n",
    "plt.scatter(X[2:, 0], X[2:, 1], marker = '+', c = 'k', s = 100, lw = 2, zorder = 10)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "clf = svm.SVC(kernel='rbf', gamma=2)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "plt.scatter(X[:2, 0], X[:2, 1], marker = '_', c = 'r', s = 100, lw = 2, zorder = 10)\n",
    "plt.scatter(X[2:, 0], X[2:, 1], marker = '+', c = 'r', s = 100, lw = 2, zorder = 10)\n",
    "\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "            levels=[-1, 0, 1])\n",
    "\n",
    "plt.axis((x_min, x_max, y_min, y_max))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SVM Handwritten Digit Recogntion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters to specify:\n",
    "\n",
    "## - kernel: The default is 'rbf'.  For other kernels, see\n",
    "\n",
    "## http://scikit-learn.org/stable/modules/svm.html#kernel-functions\n",
    "\n",
    "## - Setting C: C is 1 by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it. It corresponds to \"regularize\" more the estimation: The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth (ignores noise), while a high C aims at classifying all training examples correctly (but maybe giving noise too much weight). \n",
    "\n",
    "## - If data for classification are unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C.\n",
    "\n",
    "## - (Particular to RBF) gamma: it defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Now, we get the first \"2\" right!\n",
    "\n",
    "The first \"5\" is still trouble, \n",
    "but even human can't necessarily tell that \"5\"!  \n",
    "\n",
    "'''\n",
    "dig_data = load_digits()\n",
    "X = dig_data.data\n",
    "y = dig_data.target\n",
    "# This is basically each array in X\n",
    "# getting reshaped into (8, 8).\n",
    "dig_img = dig_data.images\n",
    "\n",
    "print(type(X), X.dtype, X.shape)\n",
    "print(type(digits), digits.dtype, digits.shape)\n",
    "\n",
    "select_idx = 1\n",
    "\n",
    "# ********************************Separating training data from testing data****************\n",
    "Xtrain = np.delete(X, select_idx, axis = 0)\n",
    "ytrain = np.delete(y, select_idx)\n",
    "\n",
    "# if you don't do .reshape(1, -1), you get a warning.\n",
    "Xtest = X[select_idx].reshape(1, -1)\n",
    "test_img = dig_img[select_idx]\n",
    "ytest = y[select_idx]\n",
    "\n",
    "print('Xtrain.shape, ytrain.shape', Xtrain.shape, ytrain.shape)\n",
    "print('Xtest.shape, ytest.shape', Xtest.shape, ytest.shape)\n",
    "\n",
    "plt.figure(figsize = (4, 4))\n",
    "plt.imshow(test_img, cmap = 'binary')\n",
    "plt.grid('off')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# ************************************* The PCA Section ********************************\n",
    "n_comp = 10\n",
    "\n",
    "pca = PCA(n_comp)  \n",
    "\n",
    "# finding pca axes\n",
    "pca.fit(Xtrain)\n",
    "# projecting training data onto pca axes\n",
    "Xtrain_proj = pca.transform(Xtrain)\n",
    "# projecting test data onto pca axes\n",
    "Xtest_proj = pca.transform(Xtest)\n",
    "\n",
    "print(Xtrain_proj.shape)\n",
    "print(Xtest_proj.shape)\n",
    "\n",
    "\n",
    "# ************************************* The SVM Section ********************************\n",
    "\n",
    "# instantiating an SVM classifier\n",
    "clf = svm.SVC(gamma=0.001, C=100.)\n",
    "\n",
    "# apply SVM to training data and draw boundaries.\n",
    "clf.fit(Xtrain_proj, ytrain)\n",
    "# Use SVM-determined boundaries to make\n",
    "# a prediction for the test data point.\n",
    "clf.predict(Xtest_proj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout:\n",
    "## 1. Turn the above into a function\n",
    "## classify_dig_svm(X, y, dig_img, select_idx, n_comp = 10, plot_test_img = False)\n",
    "## where\n",
    "## - X: data\n",
    "## - y: targes (\"ground truth\")\n",
    "## - select_idx: the index of the test data point\n",
    "## - dig_img: 2D arrays of the digit image that corresponds to select_idx\n",
    "## - plot_test_img: plot the above image, if True.\n",
    "## - n_comp: how many PCA components to use\n",
    "## 2. Test this function on select_idx = 0, 1, and 2.  One at a time.\n",
    "## 3. Write a main program that does the \"leave-one-out\" test for all 1797 images.  But start with 50 images, just as we did in last class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of week 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
